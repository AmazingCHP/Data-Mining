import pandas as pd
import json
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules
from tqdm import tqdm
import glob
import os
from å­ç±» import product_dict

# ğŸ”¹ 1. è¯»å–å•†å“ç›®å½• JSONï¼Œæ„å»ºæ˜ å°„è¡¨
print("ğŸ”¹ æ­£åœ¨åŠ è½½å•†å“ç›®å½•...")
with open("product_catalog.json", "r", encoding="utf-8") as f:
    catalog = json.load(f)
id_to_category = {item["id"]: item["category"] for item in catalog["products"]}
print("âœ… å•†å“ç›®å½•åŠ è½½å®Œæˆï¼Œå…±è®¡å•†å“æ•°ï¼š", len(id_to_category))


# ğŸ”¹ 2. å®šä¹‰ç‰¹å¾æå–å‡½æ•°
def extract_features(row):
    try:
        record = json.loads(row["purchase_history"])
        features_list = []
        items = record.get("items", [])
        payment_method = record.get("payment_method")
        payment_feature = f"{payment_method}" if payment_method else None
        for item in items:
            feature = []
            product_id = item.get("id")
            category = id_to_category.get(product_id)
            if category:
                feature.append(product_dict.get(category, category))
            if payment_feature:
                feature.append(payment_feature)
            if feature:
                features_list.append(feature)
        return features_list
    except Exception:
        return []


# ğŸ”¹ 3. åŠ è½½å¹¶é‡‡æ ·æ¯ä¸ª parquet æ–‡ä»¶
parquet_files = glob.glob(os.path.join("30G_data_new", "*.parquet"))
sampled_list = []
print(f"ğŸ”¹ å…±æ‰¾åˆ° {len(parquet_files)} ä¸ªæ•°æ®æ–‡ä»¶ï¼Œå¼€å§‹é€ä¸ªè¯»å–å¹¶é‡‡æ ·å…«åˆ†ä¹‹ä¸€...")

for i, file in enumerate(parquet_files):
    print(f"ğŸ“‚ [{i + 1}/{len(parquet_files)}] æ­£åœ¨å¤„ç†æ–‡ä»¶ï¼š{file}")
    df = pd.read_parquet(file, columns=["purchase_history"])
    df_sample = df.sample(frac=0.033, random_state=42)
    sampled_list.append(df_sample)
    print(f"   âœ… å®Œæˆé‡‡æ ·ï¼ŒåŸå§‹æ•°æ® {len(df)} æ¡ï¼Œé‡‡æ ·å {len(df_sample)} æ¡")

# ğŸ”¹ 4. åˆå¹¶æ‰€æœ‰é‡‡æ ·æ•°æ®
df_all = pd.concat(sampled_list, ignore_index=True)
print(f"ğŸ”¹ æ‰€æœ‰é‡‡æ ·åˆå¹¶å®Œæˆï¼Œæ€»è®¡è®°å½•æ•°ï¼š{len(df_all)}")

# ğŸ”¹ 5. æå–äº‹åŠ¡ç‰¹å¾
print("ğŸ”¹ æ­£åœ¨æå–äº‹åŠ¡ç‰¹å¾ï¼ˆå¸¦è¿›åº¦æ¡ï¼‰...")
tqdm.pandas(desc="â³ æå–ä¸­")
df_all["features"] = df_all.progress_apply(extract_features, axis=1)
print("âœ… ç‰¹å¾æå–å®Œæˆ")

# ğŸ”¹ 6. å±•å¹³ç‰¹å¾åˆ—è¡¨
print("ğŸ”¹ æ­£åœ¨å±•å¹³ç‰¹å¾åˆ—è¡¨...")
data = [feat for sublist in df_all["features"] for feat in sublist if feat]
print(f"âœ… ç‰¹å¾å±•å¹³å®Œæˆï¼Œå…±è®¡äº‹åŠ¡æ•°ï¼š{len(data)}")
if not data:
    print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆäº‹åŠ¡æ•°æ®ï¼Œç»ˆæ­¢åˆ†æ")
    exit()

# ğŸ”¹ 7. TransactionEncoder ç¼–ç 
print("ğŸ”¹ æ­£åœ¨è¿›è¡Œ TransactionEncoder ç¼–ç ...")
te = TransactionEncoder()
te_ary = te.fit(data).transform(data)
df_encoded = pd.DataFrame(te_ary, columns=te.columns_)
print("âœ… ç¼–ç å®Œæˆï¼Œç”Ÿæˆçš„ç»´åº¦æ•°ï¼š", len(df_encoded.columns))

# ğŸ”¹ 8. æ‰§è¡Œ Apriori åˆ†æ
print("ğŸ”¹ æ­£åœ¨æ‰§è¡Œ Apriori ç®—æ³•...")
frequent_itemsets = apriori(df_encoded, min_support=0.002, use_colnames=True)
frequent_itemsets.to_csv("é¢‘ç¹é¡¹é›†_é‡‡æ ·åˆå¹¶.csv", index=False, encoding="utf-8-sig")
print(f"âœ… é¢‘ç¹é¡¹é›†æŒ–æ˜å®Œæˆï¼Œå…±æ‰¾åˆ° {len(frequent_itemsets)} ä¸ªé¢‘ç¹é¡¹é›†")

# ğŸ”¹ 9. ç”Ÿæˆå…³è”è§„åˆ™
print("ğŸ”¹ æ­£åœ¨ç”Ÿæˆå…³è”è§„åˆ™...")
rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0)
rules = rules.sort_values(by='lift', ascending=False)
rules.to_csv("å…³è”è§„åˆ™_é‡‡æ ·åˆå¹¶.csv", index=False, encoding="utf-8-sig")
print(f"âœ… å…³è”è§„åˆ™ç”Ÿæˆå®Œæˆï¼Œå…±è®¡è§„åˆ™æ•°ï¼š{len(rules)}")

# ğŸ”¹ 10. æ‰“å°å‰å‡ æ¡ç»“æœ
print("\nğŸ“Š å‰5æ¡é¢‘ç¹é¡¹é›†ï¼š")
print(frequent_itemsets.head())

print("\nğŸ”— å‰5æ¡å…³è”è§„åˆ™ï¼š")
print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head())
